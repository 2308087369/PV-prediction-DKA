{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对基准模型的超参数进行优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\bo.chen18\\AppData\\Local\\Temp\\ipykernel_19692\\1741114340.py:5: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  file_path = \"D:\\Pythonworkshop\\Solar_data_processing\\cleaned_data_final_Yulara.csv\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集划分完成：\n",
      "训练集大小: (73040, 10)\n",
      "验证集大小: (15651, 10)\n",
      "测试集大小: (15652, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取数据\n",
    "file_path = \"D:\\Pythonworkshop\\Solar_data_processing\\cleaned_data_final_Yulara.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 提取特征和目标变量\n",
    "X = data.drop(columns=['Active_Power','timestamp'])\n",
    "y = data['Active_Power']\n",
    "\n",
    "# 按照70%、15%、15%划分训练集、验证集和测试集\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"数据集划分完成：\")\n",
    "print(f\"训练集大小: {X_train.shape}\")\n",
    "print(f\"验证集大小: {X_val.shape}\")\n",
    "print(f\"测试集大小: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 将特征和目标变量按照序列移动，形成单步预测\n",
    "def create_sequences(X, y, n_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - n_steps):\n",
    "        Xs.append(X.iloc[i:(i + n_steps)].values)\n",
    "        ys.append(y.iloc[i + n_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# 使用 n_steps = 1 (单步预测)\n",
    "n_steps = 1\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, n_steps)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, n_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, n_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not all points are within the bounds of the space.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m mlp_opt \u001b[38;5;241m=\u001b[39m BayesSearchCV(estimator\u001b[38;5;241m=\u001b[39mmlp, search_spaces\u001b[38;5;241m=\u001b[39mmlp_search_space, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 进行搜索并获取最佳参数\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mmlp_opt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters found for MLP:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mlp_opt\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest score for MLP:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mlp_opt\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[1;32mc:\\Users\\bo.chen18\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\searchcv.py:542\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[1;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit):\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBayesSearchCV doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support a callable refit, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt define an implicit score to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    540\u001b[0m     )\n\u001b[1;32m--> 542\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[1;32mc:\\Users\\bo.chen18\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bo.chen18\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\bo.chen18\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\searchcv.py:599\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     n_points_adjusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[1;32m--> 599\u001b[0m     optim_result, score_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points_adjusted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[1;32mc:\\Users\\bo.chen18\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\searchcv.py:482\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[1;34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001b[0m\n\u001b[0;32m    479\u001b[0m local_results \u001b[38;5;241m=\u001b[39m all_results[score_name][\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(params) :]\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# return the score_name to cache it if callable refit\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# this avoids checking self.refit all the time\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal_results\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, score_name)\n",
      "File \u001b[1;32mc:\\Users\\bo.chen18\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:559\u001b[0m, in \u001b[0;36mOptimizer.tell\u001b[1;34m(self, x, y, fit)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtell\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    533\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Record an observation (or several) of the objective function.\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \n\u001b[0;32m    535\u001b[0m \u001b[38;5;124;03m    Provide values of the objective function at points suggested by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;124;03m        the optimizer irrespective of the value of `fit`.\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m     \u001b[43mcheck_x_in_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_y_is_valid(x, y)\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;66;03m# take the logarithm of the computation times\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bo.chen18\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skopt\\utils.py:184\u001b[0m, in \u001b[0;36mcheck_x_in_space\u001b[1;34m(x, space)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_2Dlistlike(x):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall([p \u001b[38;5;129;01min\u001b[39;00m space \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m x]):\n\u001b[1;32m--> 184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot all points are within the bounds of\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the space.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([\u001b[38;5;28mlen\u001b[39m(p) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(space\u001b[38;5;241m.\u001b[39mdimensions) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m x]):\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot all points have the same dimensions as\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the space.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Not all points are within the bounds of the space."
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical\n",
    "\n",
    "# 定义MLP的超参数搜索空间\n",
    "mlp_search_space = {\n",
    "    'hidden_layer_sizes': Categorical([(50,), (100,)]),\n",
    "    'activation': Categorical(['relu']),\n",
    "    'solver': Categorical(['adam']),\n",
    "    'alpha': Real(1e-5, 1e-2, prior='log-uniform'),  # 调整alpha的范围，确保数值在合理范围内\n",
    "    'learning_rate': Categorical(['constant']),\n",
    "    'learning_rate_init': Real(1e-3, 1e-1, prior='log-uniform')  # 调整learning_rate_init的范围\n",
    "}\n",
    "\n",
    "\n",
    "# 初始化MLP模型\n",
    "mlp = MLPRegressor(max_iter=1000, random_state=42)\n",
    "\n",
    "# 使用贝叶斯优化进行超参数搜索\n",
    "mlp_opt = BayesSearchCV(estimator=mlp, search_spaces=mlp_search_space, n_iter=32, cv=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 进行搜索并获取最佳参数\n",
    "mlp_opt.fit(X_train, y_train)\n",
    "print(\"Best parameters found for MLP:\", mlp_opt.best_params_)\n",
    "print(\"Best score for MLP:\", mlp_opt.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# 定义SVM的超参数搜索空间\n",
    "svm_search_space = {\n",
    "    'C': Real(1e-6, 1e+2, prior='log-uniform'),\n",
    "    'epsilon': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "    'kernel': Categorical(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'degree': Integer(1, 5),  # 适用于'poly' kernel\n",
    "    'gamma': Categorical(['scale', 'auto']),\n",
    "}\n",
    "\n",
    "# 初始化SVM模型\n",
    "svm = SVR()\n",
    "\n",
    "# 使用贝叶斯优化进行超参数搜索\n",
    "svm_opt = BayesSearchCV(estimator=svm, search_spaces=svm_search_space, n_iter=32, cv=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 进行搜索并获取最佳参数\n",
    "svm_opt.fit(X_train, y_train)\n",
    "print(\"Best parameters found for SVM:\", svm_opt.best_params_)\n",
    "print(\"Best score for SVM:\", svm_opt.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "def build_lstm_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=128, step=16), \n",
    "                   activation='relu', \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),\n",
    "                  loss='mse')\n",
    "    return model\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    build_lstm_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='lstm_tuner',\n",
    "    project_name='lstm_bayes_opt'\n",
    ")\n",
    "\n",
    "# 执行贝叶斯优化\n",
    "tuner.search(X_train_seq, y_train_seq, epochs=10, validation_data=(X_val_seq, y_val_seq))\n",
    "\n",
    "# 获取最优模型\n",
    "best_lstm_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters for LSTM:\", best_hyperparameters.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "def build_cnn_lstm_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=hp.Int('filters', min_value=32, max_value=128, step=16), \n",
    "                     kernel_size=hp.Int('kernel_size', min_value=1, max_value=5), \n",
    "                     activation='relu', \n",
    "                     input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=128, step=16), activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),\n",
    "                  loss='mse')\n",
    "    return model\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    build_cnn_lstm_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='cnn_lstm_tuner',\n",
    "    project_name='cnn_lstm_bayes_opt'\n",
    ")\n",
    "\n",
    "# 执行贝叶斯优化\n",
    "tuner.search(X_train_seq, y_train_seq, epochs=10, validation_data=(X_val_seq, y_val_seq))\n",
    "\n",
    "# 获取最优模型\n",
    "best_cnn_lstm_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters_cnn_lstm = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters for CNN-LSTM:\", best_hyperparameters_cnn_lstm.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最优参数训练MLP\n",
    "best_mlp = mlp_opt.best_estimator_\n",
    "best_mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = best_mlp.predict(X_test)\n",
    "\n",
    "# 使用最优参数训练SVM\n",
    "best_svm = svm_opt.best_estimator_\n",
    "best_svm.fit(X_train, y_train)\n",
    "y_pred_svm = best_svm.predict(X_test)\n",
    "\n",
    "# 使用最优LSTM模型进行预测\n",
    "y_pred_lstm = best_lstm_model.predict(X_test_seq).flatten()\n",
    "\n",
    "# 使用最优CNN-LSTM模型进行预测\n",
    "y_pred_cnn_lstm = best_cnn_lstm_model.predict(X_test_seq).flatten()\n",
    "\n",
    "# 评估优化后的模型\n",
    "mae_mlp, mse_mlp, rmse_mlp, nrmse_mlp = evaluate_model(y_test, y_pred_mlp)\n",
    "mae_svm, mse_svm, rmse_svm, nrmse_svm = evaluate_model(y_test, y_pred_svm)\n",
    "mae_lstm, mse_lstm, rmse_lstm, nrmse_lstm = evaluate_model(y_test_seq, y_pred_lstm)\n",
    "mae_cnn_lstm, mse_cnn_lstm, rmse_cnn_lstm, nrmse_cnn_lstm = evaluate_model(y_test_seq, y_pred_cnn_lstm)\n",
    "\n",
    "print(f\"Optimized MLP - MAE: {mae_mlp}, MSE: {mse_mlp}, RMSE: {rmse_mlp}, NRMSE: {nrmse_mlp}\")\n",
    "print(f\"Optimized SVM - MAE: {mae_svm}, MSE: {mse_svm}, RMSE: {rmse_svm}, NRMSE: {nrmse_svm}\")\n",
    "print(f\"Optimized LSTM - MAE: {mae_lstm}, MSE: {mse_lstm}, RMSE: {rmse_lstm}, NRMSE: {nrmse_lstm}\")\n",
    "print(f\"Optimized CNN-LSTM - MAE: {mae_cnn_lstm}, MSE: {mse_cnn_lstm}, RMSE: {rmse_cnn_lstm}, NRMSE: {nrmse_cnn_lstm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
